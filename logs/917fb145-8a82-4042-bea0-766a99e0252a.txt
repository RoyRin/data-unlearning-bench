import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_bin = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_bin = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    max_device_batch_size = 64*1024 # batch size per device in tokens
    num_iterations = 1390 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # NEW FIELDS FOR FLEXIBLE TRAINING SETUP
    num_gpus = 8  # expected number of GPUs / distributed processes
    grad_accum_steps = 1  # gradient accumulation steps per optimizer update
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # implementation
    save_checkpoint = False
args = Hyperparameters()
# Override from environment variables if provided (allows launch script to change without code edit)
args.num_gpus = int(os.environ.get('NUM_GPUS', args.num_gpus))
args.grad_accum_steps = int(os.environ.get('GRAD_ACCUM_STEPS', args.grad_accum_steps))

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert world_size == args.num_gpus, f"Mismatch between launched processes ({world_size}) and args.num_gpus ({args.num_gpus})"
assert args.batch_size % (world_size * args.grad_accum_steps) == 0, "batch_size must be divisible by world_size * grad_accum_steps to keep effective batch size constant"
per_device_tokens = args.batch_size // (world_size * args.grad_accum_steps)
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_bin)
val_loader = DistributedDataLoader(args.val_bin)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()

    global_tokens_per_accum = args.batch_size // args.grad_accum_steps  # tokens processed per accumulation step across all GPUs
    assert global_tokens_per_accum % world_size == 0
    for accum_idx in range(args.grad_accum_steps):
        inputs_train, targets_train = train_loader.next_batch(global_tokens_per_accum)
        # ensure we can split into micro batches that fit in memory
        assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
        for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
            loss = ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks) / args.grad_accum_steps
            loss.backward()

    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.8.0.dev20250610+cu126 compiled for CUDA 12.6
Fri Jun 27 17:15:22 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:07:00.0 Off |                    0 |
| N/A   29C    P0             67W /  400W |    3120MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:0B:00.0 Off |                    0 |
| N/A   29C    P0             73W /  400W |    1101MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:48:00.0 Off |                    0 |
| N/A   46C    P0            260W /  400W |   51975MiB /  81920MiB |    100%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   43C    P0            168W /  400W |   15558MiB /  81920MiB |     98%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA A100-SXM4-80GB          On  |   00000000:88:00.0 Off |                    0 |
| N/A   44C    P0            220W /  400W |   29157MiB /  81920MiB |    100%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA A100-SXM4-80GB          On  |   00000000:8B:00.0 Off |                    0 |
| N/A   53C    P0            338W /  400W |   37313MiB /  81920MiB |    100%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA A100-SXM4-80GB          On  |   00000000:C8:00.0 Off |                    0 |
| N/A   42C    P0            250W /  400W |   22667MiB /  81920MiB |    100%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA A100-SXM4-80GB          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   43C    P0            204W /  400W |   21563MiB /  81920MiB |     99%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   2452139      C   ...aarshia/.conda/envs/work/bin/python        522MiB |
|    0   N/A  N/A   3190293      C   .../.conda/envs/nanogpt/bin/python3.12       1090MiB |
|    0   N/A  N/A   3190294      C   .../.conda/envs/nanogpt/bin/python3.12        492MiB |
|    0   N/A  N/A   3190295      C   .../.conda/envs/nanogpt/bin/python3.12        492MiB |
|    0   N/A  N/A   3190296      C   .../.conda/envs/nanogpt/bin/python3.12        492MiB |
|    1   N/A  N/A   3190294      C   .../.conda/envs/nanogpt/bin/python3.12       1090MiB |
|    2   N/A  N/A    951052      C   ...wu/miniconda3/envs/rulet/bin/python      51382MiB |
|    3   N/A  N/A   3171384      C   python                                       5592MiB |
|    3   N/A  N/A   3176151      C   python                                       5590MiB |
|    3   N/A  N/A   3190295      C   .../.conda/envs/nanogpt/bin/python3.12       1090MiB |
|    4   N/A  N/A   2675913      C   ...iniconda3/envs/openmmlab/bin/python      20226MiB |
|    4   N/A  N/A   3176675      C   python                                       4440MiB |
|    4   N/A  N/A   3177690      C   python                                       4438MiB |
|    5   N/A  N/A   2675914      C   ...iniconda3/envs/openmmlab/bin/python      20242MiB |
|    5   N/A  N/A   3161078      C   python                                       8508MiB |
|    5   N/A  N/A   3166232      C   python                                       8508MiB |
|    6   N/A  N/A   3107928      C   python                                      10760MiB |
|    6   N/A  N/A   3117536      C   python                                      10762MiB |
|    6   N/A  N/A   3190296      C   .../.conda/envs/nanogpt/bin/python3.12       1090MiB |
|    7   N/A  N/A   3151703      C   python                                      10758MiB |
|    7   N/A  N/A   3151909      C   python                                      10758MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1390 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1390 train_time:119758ms step_avg:nanms
step:2/1390 train_time:121033ms step_avg:nanms
step:3/1390 train_time:122044ms step_avg:nanms
step:4/1390 train_time:123057ms step_avg:nanms
step:5/1390 train_time:124035ms step_avg:nanms
step:6/1390 train_time:125041ms step_avg:nanms
step:7/1390 train_time:126049ms step_avg:nanms
step:8/1390 train_time:127025ms step_avg:nanms
step:9/1390 train_time:128037ms step_avg:nanms
step:10/1390 train_time:129047ms step_avg:nanms
step:11/1390 train_time:986ms step_avg:nanms
step:12/1390 train_time:1992ms step_avg:nanms
step:13/1390 train_time:2994ms step_avg:998.11ms
step:14/1390 train_time:3988ms step_avg:997.07ms
step:15/1390 train_time:5005ms step_avg:1001.04ms
step:16/1390 train_time:5999ms step_avg:999.91ms
step:17/1390 train_time:6988ms step_avg:998.36ms
step:18/1390 train_time:8004ms step_avg:1000.52ms
step:19/1390 train_time:8995ms step_avg:999.41ms
step:20/1390 train_time:9996ms step_avg:999.63ms
step:21/1390 train_time:11010ms step_avg:1000.94ms
step:22/1390 train_time:12005ms step_avg:1000.40ms
step:23/1390 train_time:12998ms step_avg:999.85ms
step:24/1390 train_time:14006ms step_avg:1000.42ms
step:25/1390 train_time:14998ms step_avg:999.84ms
step:26/1390 train_time:16013ms step_avg:1000.79ms
step:27/1390 train_time:17009ms step_avg:1000.51ms
step:28/1390 train_time:18002ms step_avg:1000.13ms
step:29/1390 train_time:19025ms step_avg:1001.32ms
step:30/1390 train_time:20018ms step_avg:1000.89ms
step:31/1390 train_time:21021ms step_avg:1001.01ms
step:32/1390 train_time:22028ms step_avg:1001.25ms
step:33/1390 train_time:23033ms step_avg:1001.42ms
step:34/1390 train_time:24028ms step_avg:1001.18ms
step:35/1390 train_time:25047ms step_avg:1001.89ms
step:36/1390 train_time:26045ms step_avg:1001.74ms
step:37/1390 train_time:27063ms step_avg:1002.32ms
step:38/1390 train_time:28054ms step_avg:1001.91ms
step:39/1390 train_time:29045ms step_avg:1001.56ms
step:40/1390 train_time:30053ms step_avg:1001.77ms
step:41/1390 train_time:31061ms step_avg:1001.96ms
step:42/1390 train_time:32051ms step_avg:1001.59ms
step:43/1390 train_time:33060ms step_avg:1001.82ms
step:44/1390 train_time:34062ms step_avg:1001.83ms
step:45/1390 train_time:35062ms step_avg:1001.76ms
step:46/1390 train_time:36068ms step_avg:1001.90ms
step:47/1390 train_time:37050ms step_avg:1001.35ms
step:48/1390 train_time:38098ms step_avg:1002.58ms
step:49/1390 train_time:39089ms step_avg:1002.27ms
step:50/1390 train_time:40077ms step_avg:1001.93ms
step:51/1390 train_time:41102ms step_avg:1002.49ms
step:52/1390 train_time:42101ms step_avg:1002.41ms
step:53/1390 train_time:43096ms step_avg:1002.24ms
step:54/1390 train_time:44126ms step_avg:1002.86ms
step:55/1390 train_time:45140ms step_avg:1003.11ms
step:56/1390 train_time:46109ms step_avg:1002.36ms
step:57/1390 train_time:47136ms step_avg:1002.89ms
step:58/1390 train_time:48154ms step_avg:1003.22ms
step:59/1390 train_time:49120ms step_avg:1002.45ms
step:60/1390 train_time:50139ms step_avg:1002.78ms
step:61/1390 train_time:51170ms step_avg:1003.34ms
step:62/1390 train_time:52161ms step_avg:1003.10ms
step:63/1390 train_time:53176ms step_avg:1003.33ms
step:64/1390 train_time:54196ms step_avg:1003.62ms
step:65/1390 train_time:55193ms step_avg:1003.52ms
step:66/1390 train_time:56197ms step_avg:1003.51ms
step:67/1390 train_time:57218ms step_avg:1003.82ms
step:68/1390 train_time:58206ms step_avg:1003.56ms
step:69/1390 train_time:59196ms step_avg:1003.33ms
step:70/1390 train_time:60238ms step_avg:1003.97ms
step:71/1390 train_time:61214ms step_avg:1003.51ms
step:72/1390 train_time:62204ms step_avg:1003.29ms
step:73/1390 train_time:63240ms step_avg:1003.81ms
step:74/1390 train_time:64240ms step_avg:1003.75ms
step:75/1390 train_time:65242ms step_avg:1003.73ms
step:76/1390 train_time:66295ms step_avg:1004.47ms
step:77/1390 train_time:67306ms step_avg:1004.57ms
step:78/1390 train_time:68698ms step_avg:1010.26ms
step:79/1390 train_time:70022ms step_avg:1014.81ms
step:80/1390 train_time:71330ms step_avg:1019.00ms
step:81/1390 train_time:72637ms step_avg:1023.05ms
step:82/1390 train_time:73963ms step_avg:1027.27ms
step:83/1390 train_time:75290ms step_avg:1031.37ms
step:84/1390 train_time:76605ms step_avg:1035.20ms
step:85/1390 train_time:77926ms step_avg:1039.01ms
step:86/1390 train_time:79240ms step_avg:1042.64ms
step:87/1390 train_time:80556ms step_avg:1046.18ms
step:88/1390 train_time:81860ms step_avg:1049.49ms
step:89/1390 train_time:83198ms step_avg:1053.13ms
step:90/1390 train_time:84523ms step_avg:1056.54ms
step:91/1390 train_time:85843ms step_avg:1059.79ms
step:92/1390 train_time:87148ms step_avg:1062.78ms
step:93/1390 train_time:88464ms step_avg:1065.84ms
step:94/1390 train_time:89781ms step_avg:1068.82ms
step:95/1390 train_time:91022ms step_avg:1070.85ms
step:96/1390 train_time:92352ms step_avg:1073.86ms
step:97/1390 train_time:93660ms step_avg:1076.56ms
step:98/1390 train_time:94989ms step_avg:1079.42ms
step:99/1390 train_time:96317ms step_avg:1082.21ms
step:100/1390 train_time:97646ms step_avg:1084.96ms
step:101/1390 train_time:98965ms step_avg:1087.53ms
step:102/1390 train_time:100300ms step_avg:1090.21ms
step:103/1390 train_time:101653ms step_avg:1093.05ms
step:104/1390 train_time:103066ms step_avg:1096.45ms
step:105/1390 train_time:104477ms step_avg:1099.76ms
step:106/1390 train_time:105897ms step_avg:1103.09ms
step:107/1390 train_time:107312ms step_avg:1106.31ms
step:108/1390 train_time:108702ms step_avg:1109.21ms
step:109/1390 train_time:110102ms step_avg:1112.14ms
step:110/1390 train_time:111515ms step_avg:1115.15ms
step:111/1390 train_time:112848ms step_avg:1117.31ms
step:112/1390 train_time:114266ms step_avg:1120.26ms
step:113/1390 train_time:115692ms step_avg:1123.22ms
step:114/1390 train_time:117133ms step_avg:1126.27ms
step:115/1390 train_time:118540ms step_avg:1128.96ms
step:116/1390 train_time:119943ms step_avg:1131.53ms
step:117/1390 train_time:121347ms step_avg:1134.08ms
step:118/1390 train_time:122756ms step_avg:1136.63ms
step:119/1390 train_time:124183ms step_avg:1139.29ms
step:120/1390 train_time:125594ms step_avg:1141.76ms
step:121/1390 train_time:126986ms step_avg:1144.02ms
step:122/1390 train_time:128386ms step_avg:1146.30ms
step:123/1390 train_time:129787ms step_avg:1148.55ms
step:124/1390 train_time:131203ms step_avg:1150.91ms
step:125/1390 train_time:132624ms step_avg:1153.26ms
step:125/1390 val_loss:4.3715 train_time:132768ms step_avg:1154.50ms
step:126/1390 train_time:133551ms step_avg:1151.30ms
step:127/1390 train_time:134651ms step_avg:1150.86ms
step:128/1390 train_time:135723ms step_avg:1150.19ms
step:129/1390 train_time:136774ms step_avg:1149.36ms
step:130/1390 train_time:137856ms step_avg:1148.80ms
step:131/1390 train_time:138953ms step_avg:1148.37ms
step:132/1390 train_time:139978ms step_avg:1147.36ms
step:133/1390 train_time:141045ms step_avg:1146.71ms
step:134/1390 train_time:142158ms step_avg:1146.43ms
step:135/1390 train_time:143234ms step_avg:1145.87ms
step:136/1390 train_time:144288ms step_avg:1145.14ms
step:137/1390 train_time:145379ms step_avg:1144.71ms
step:138/1390 train_time:146477ms step_avg:1144.35ms
step:139/1390 train_time:147545ms step_avg:1143.76ms
step:140/1390 train_time:148606ms step_avg:1143.12ms
step:141/1390 train_time:149699ms step_avg:1142.74ms
step:142/1390 train_time:150792ms step_avg:1142.36ms
step:143/1390 train_time:151852ms step_avg:1141.74ms
step:144/1390 train_time:152928ms step_avg:1141.25ms
step:145/1390 train_time:154037ms step_avg:1141.02ms
step:146/1390 train_time:155125ms step_avg:1140.62ms
step:147/1390 train_time:156183ms step_avg:1140.02ms
step:148/1390 train_time:157264ms step_avg:1139.59ms
step:149/1390 train_time:158360ms step_avg:1139.28ms
step:150/1390 train_time:159440ms step_avg:1138.86ms
step:151/1390 train_time:160501ms step_avg:1138.30ms
step:152/1390 train_time:161590ms step_avg:1137.96ms
step:153/1390 train_time:162700ms step_avg:1137.76ms
step:154/1390 train_time:163757ms step_avg:1137.20ms
step:155/1390 train_time:164826ms step_avg:1136.73ms
step:156/1390 train_time:165936ms step_avg:1136.55ms
step:157/1390 train_time:167024ms step_avg:1136.22ms
step:158/1390 train_time:168083ms step_avg:1135.69ms
step:159/1390 train_time:169158ms step_avg:1135.29ms
step:160/1390 train_time:170260ms step_avg:1135.06ms
step:161/1390 train_time:171347ms step_avg:1134.75ms
step:162/1390 train_time:172397ms step_avg:1134.19ms
step:163/1390 train_time:173481ms step_avg:1133.86ms
step:164/1390 train_time:174587ms step_avg:1133.68ms
step:165/1390 train_time:175656ms step_avg:1133.27ms
step:166/1390 train_time:176727ms step_avg:1132.87ms
step:167/1390 train_time:177824ms step_avg:1132.64ms
step:168/1390 train_time:178905ms step_avg:1132.31ms
step:169/1390 train_time:179965ms step_avg:1131.85ms
step:170/1390 train_time:181046ms step_avg:1131.54ms
step:171/1390 train_time:182144ms step_avg:1131.33ms
step:172/1390 train_time:183225ms step_avg:1131.02ms
step:173/1390 train_time:184286ms step_avg:1130.59ms
step:174/1390 train_time:185374ms step_avg:1130.33ms
step:175/1390 train_time:186460ms step_avg:1130.06ms
step:176/1390 train_time:187553ms step_avg:1129.84ms
step:177/1390 train_time:188614ms step_avg:1129.43ms
step:178/1390 train_time:189698ms step_avg:1129.16ms
step:179/1390 train_time:190776ms step_avg:1128.85ms
step:180/1390 train_time:191849ms step_avg:1128.52ms
step:181/1390 train_time:192924ms step_avg:1128.21ms
step:182/1390 train_time:194005ms step_avg:1127.94ms
step:183/1390 train_time:195089ms step_avg:1127.68ms
step:184/1390 train_time:196163ms step_avg:1127.37ms
step:185/1390 train_time:197250ms step_avg:1127.15ms
step:186/1390 train_time:198339ms step_avg:1126.93ms
step:187/1390 train_time:199422ms step_avg:1126.68ms
step:188/1390 train_time:200484ms step_avg:1126.32ms
step:189/1390 train_time:201580ms step_avg:1126.14ms
step:190/1390 train_time:202666ms step_avg:1125.92ms
step:191/1390 train_time:203749ms step_avg:1125.69ms
step:192/1390 train_time:204820ms step_avg:1125.38ms
step:193/1390 train_time:205908ms step_avg:1125.18ms
step:194/1390 train_time:206992ms step_avg:1124.96ms
step:195/1390 train_time:208060ms step_avg:1124.65ms
step:196/1390 train_time:209142ms step_avg:1124.42ms
step:197/1390 train_time:210219ms step_avg:1124.17ms
step:198/1390 train_time:211300ms step_avg:1123.93ms
step:199/1390 train_time:212364ms step_avg:1123.62ms
step:200/1390 train_time:213455ms step_avg:1123.45ms
step:201/1390 train_time:214530ms step_avg:1123.19ms
step:202/1390 train_time:215612ms step_avg:1122.98ms
step:203/1390 train_time:216680ms step_avg:1122.69ms
step:204/1390 train_time:217767ms step_avg:1122.51ms
step:205/1390 train_time:218850ms step_avg:1122.31ms
step:206/1390 train_time:219917ms step_avg:1122.03ms
step:207/1390 train_time:221039ms step_avg:1122.03ms
step:208/1390 train_time:222190ms step_avg:1122.17ms
step:209/1390 train_time:223323ms step_avg:1122.23ms
step:210/1390 train_time:224457ms step_avg:1122.29ms
step:211/1390 train_time:225591ms step_avg:1122.34ms
step:212/1390 train_time:226741ms step_avg:1122.48ms
step:213/1390 train_time:227880ms step_avg:1122.56ms
step:214/1390 train_time:229029ms step_avg:1122.69ms
step:215/1390 train_time:230141ms step_avg:1122.64ms
step:216/1390 train_time:231283ms step_avg:1122.73ms
step:217/1390 train_time:232422ms step_avg:1122.81ms
step:218/1390 train_time:233576ms step_avg:1122.96ms
step:219/1390 train_time:234697ms step_avg:1122.95ms
step:220/1390 train_time:235836ms step_avg:1123.03ms
step:221/1390 train_time:236978ms step_avg:1123.12ms
step:222/1390 train_time:238118ms step_avg:1123.20ms
step:223/1390 train_time:239259ms step_avg:1123.28ms
step:224/1390 train_time:240391ms step_avg:1123.32ms
step:225/1390 train_time:241528ms step_avg:1123.38ms
step:226/1390 train_time:242672ms step_avg:1123.48ms
step:227/1390 train_time:243816ms step_avg:1123.58ms
step:228/1390 train_time:244929ms step_avg:1123.53ms
step:229/1390 train_time:246071ms step_avg:1123.61ms
step:230/1390 train_time:247202ms step_avg:1123.65ms
step:231/1390 train_time:248344ms step_avg:1123.73ms
step:232/1390 train_time:249483ms step_avg:1123.80ms
step:233/1390 train_time:250645ms step_avg:1123.97ms
step:234/1390 train_time:251758ms step_avg:1123.92ms
step:235/1390 train_time:252918ms step_avg:1124.08ms
step:236/1390 train_time:254047ms step_avg:1124.10ms
step:237/1390 train_time:255197ms step_avg:1124.22ms
step:238/1390 train_time:256323ms step_avg:1124.22ms
step:239/1390 train_time:257459ms step_avg:1124.28ms
step:240/1390 train_time:258608ms step_avg:1124.38ms
step:241/1390 train_time:259761ms step_avg:1124.51ms
step:242/1390 train_time:260907ms step_avg:1124.60ms
step:243/1390 train_time:262020ms step_avg:1124.55ms
step:244/1390 train_time:263174ms step_avg:1124.67ms
step:245/1390 train_time:264323ms step_avg:1124.78ms
step:246/1390 train_time:265473ms step_avg:1124.89ms
step:247/1390 train_time:266589ms step_avg:1124.85ms
step:248/1390 train_time:267732ms step_avg:1124.92ms
step:249/1390 train_time:268888ms step_avg:1125.05ms
step:250/1390 train_time:270027ms step_avg:1125.11ms
step:250/1390 val_loss:3.9444 train_time:270297ms step_avg:1126.24ms
step:251/1390 train_time:271156ms step_avg:1125.13ms
step:252/1390 train_time:272316ms step_avg:1125.27ms
step:253/1390 train_time:273458ms step_avg:1125.34ms
step:254/1390 train_time:274581ms step_avg:1125.33ms
step:255/1390 train_time:275714ms step_avg:1125.36ms
step:256/1390 train_time:276860ms step_avg:1125.45ms
step:257/1390 train_time:278005ms step_avg:1125.53ms
step:258/1390 train_time:279153ms step_avg:1125.62ms
step:259/1390 train_time:280271ms step_avg:1125.59ms
step:260/1390 train_time:281410ms step_avg:1125.64ms
step:261/1390 train_time:282565ms step_avg:1125.76ms
step:262/1390 train_time:283711ms step_avg:1125.84ms
step:263/1390 train_time:284824ms step_avg:1125.79ms
step:264/1390 train_time:285981ms step_avg:1125.91ms
step:265/1390 train_time:287124ms step_avg:1125.98ms
step:266/1390 train_time:288275ms step_avg:1126.07ms
step:267/1390 train_time:289400ms step_avg:1126.07ms
step:268/1390 train_time:290535ms step_avg:1126.10ms
step:269/1390 train_time:291677ms step_avg:1126.17ms
step:270/1390 train_time:292847ms step_avg:1126.33ms
step:271/1390 train_time:293979ms step_avg:1126.36ms
step:272/1390 train_time:295098ms step_avg:1126.33ms
step:273/1390 train_time:296247ms step_avg:1126.41ms
step:274/1390 train_time:297397ms step_avg:1126.50ms
step:275/1390 train_time:298543ms step_avg:1126.58ms
step:276/1390 train_time:299674ms step_avg:1126.59ms
step:277/1390 train_time:300801ms step_avg:1126.59ms
step:278/1390 train_time:301950ms step_avg:1126.68ms
step:279/1390 train_time:303115ms step_avg:1126.82ms
step:280/1390 train_time:304274ms step_avg:1126.94ms
step:281/1390 train_time:305385ms step_avg:1126.88ms
step:282/1390 train_time:306526ms step_avg:1126.93ms
step:283/1390 train_time:307694ms step_avg:1127.08ms
step:284/1390 train_time:308834ms step_avg:1127.13ms
step:285/1390 train_time:309957ms step_avg:1127.12ms
step:286/1390 train_time:311095ms step_avg:1127.16ms
step:287/1390 train_time:312247ms step_avg:1127.25ms
step:288/1390 train_time:313403ms step_avg:1127.35ms
step:289/1390 train_time:314542ms step_avg:1127.39ms
step:290/1390 train_time:315672ms step_avg:1127.40ms
step:291/1390 train_time:316838ms step_avg:1127.54ms
step:292/1390 train_time:317994ms step_avg:1127.64ms
step:293/1390 train_time:319138ms step_avg:1127.70ms
step:294/1390 train_time:320247ms step_avg:1127.63ms
step:295/1390 train_time:321364ms step_avg:1127.59ms
step:296/1390 train_time:322533ms step_avg:1127.74ms
step:297/1390 train_time:323704ms step_avg:1127.89ms
step:298/1390 train_time:324822ms step_avg:1127.85ms
step:299/1390 train_time:325980ms step_avg:1127.96ms
step:300/1390 train_time:327128ms step_avg:1128.03ms
step:301/1390 train_time:328301ms step_avg:1128.18ms
step:302/1390 train_time:329420ms step_avg:1128.15ms
step:303/1390 train_time:330543ms step_avg:1128.13ms
step:304/1390 train_time:331676ms step_avg:1128.15ms
step:305/1390 train_time:332844ms step_avg:1128.28ms
step:306/1390 train_time:334007ms step_avg:1128.40ms
step:307/1390 train_time:335112ms step_avg:1128.32ms
step:308/1390 train_time:336246ms step_avg:1128.34ms
step:309/1390 train_time:337420ms step_avg:1128.50ms
step:310/1390 train_time:338619ms step_avg:1128.73ms
step:311/1390 train_time:339810ms step_avg:1128.94ms
step:312/1390 train_time:340987ms step_avg:1129.10ms
step:313/1390 train_time:342158ms step_avg:1129.24ms
step:314/1390 train_time:343355ms step_avg:1129.46ms
step:315/1390 train_time:344546ms step_avg:1129.66ms
step:316/1390 train_time:345736ms step_avg:1129.85ms
step:317/1390 train_time:346910ms step_avg:1130.00ms
step:318/1390 train_time:348102ms step_avg:1130.20ms
step:319/1390 train_time:349306ms step_avg:1130.44ms
step:320/1390 train_time:350501ms step_avg:1130.65ms
step:321/1390 train_time:351674ms step_avg:1130.79ms
step:322/1390 train_time:352851ms step_avg:1130.93ms
step:323/1390 train_time:354036ms step_avg:1131.11ms
step:324/1390 train_time:355244ms step_avg:1131.35ms
step:325/1390 train_time:356441ms step_avg:1131.56ms
step:326/1390 train_time:357641ms step_avg:1131.78ms
step:327/1390 train_time:358811ms step_avg:1131.90ms
step:328/1390 train_time:359984ms step_avg:1132.03ms
step:329/1390 train_time:361200ms step_avg:1132.29ms
step:330/1390 train_time:362422ms step_avg:1132.57ms
step:331/1390 train_time:363615ms step_avg:1132.76ms
step:332/1390 train_time:364784ms step_avg:1132.87ms
step:333/1390 train_time:365969ms step_avg:1133.03ms
step:334/1390 train_time:367167ms step_avg:1133.23ms
step:335/1390 train_time:368372ms step_avg:1133.45ms
step:336/1390 train_time:369565ms step_avg:1133.64ms
step:337/1390 train_time:370731ms step_avg:1133.73ms
step:338/1390 train_time:371900ms step_avg:1133.84ms
step:339/1390 train_time:373090ms step_avg:1134.01ms
step:340/1390 train_time:374289ms step_avg:1134.21ms
step:341/1390 train_time:375477ms step_avg:1134.37ms
step:342/1390 train_time:376662ms step_avg:1134.53ms
step:343/1390 train_time:377846ms step_avg:1134.67ms
step:344/1390 train_time:379026ms step_avg:1134.81ms
step:345/1390 train_time:380242ms step_avg:1135.05ms
step:346/1390 train_time:381432ms step_avg:1135.21ms
step:347/1390 train_time:382609ms step_avg:1135.34ms
step:348/1390 train_time:383793ms step_avg:1135.48ms
step:349/1390 train_time:384998ms step_avg:1135.69ms
step:350/1390 train_time:386201ms step_avg:1135.89ms
step:351/1390 train_time:387396ms step_avg:1136.06ms
step:352/1390 train_time:388564ms step_avg:1136.15ms
step:353/1390 train_time:389734ms step_avg:1136.25ms
step:354/1390 train_time:390928ms step_avg:1136.42ms
step:355/1390 train_time:392148ms step_avg:1136.66ms
step:356/1390 train_time:393345ms step_avg:1136.83ms
step:357/1390 train_time:394535ms step_avg:1136.99ms
step:358/1390 train_time:395721ms step_avg:1137.13ms
step:359/1390 train_time:396913ms step_avg:1137.29ms
step:360/1390 train_time:398123ms step_avg:1137.49ms
step:361/1390 train_time:399337ms step_avg:1137.71ms
step:362/1390 train_time:400511ms step_avg:1137.81ms
step:363/1390 train_time:401679ms step_avg:1137.90ms
step:364/1390 train_time:402865ms step_avg:1138.04ms
step:365/1390 train_time:404066ms step_avg:1138.21ms
step:366/1390 train_time:405281ms step_avg:1138.43ms
step:367/1390 train_time:406463ms step_avg:1138.55ms
step:368/1390 train_time:407642ms step_avg:1138.66ms
step:369/1390 train_time:408811ms step_avg:1138.75ms
step:370/1390 train_time:410013ms step_avg:1138.93ms
step:371/1390 train_time:411220ms step_avg:1139.11ms
step:372/1390 train_time:412416ms step_avg:1139.27ms
step:373/1390 train_time:413586ms step_avg:1139.36ms
step:374/1390 train_time:414769ms step_avg:1139.48ms
step:375/1390 train_time:415969ms step_avg:1139.64ms
step:375/1390 val_loss:3.7676 train_time:416258ms step_avg:1140.43ms
step:376/1390 train_time:417179ms step_avg:1139.83ms
step:377/1390 train_time:418349ms step_avg:1139.92ms
step:378/1390 train_time:419533ms step_avg:1140.03ms
step:379/1390 train_time:420748ms step_avg:1140.24ms
step:380/1390 train_time:421960ms step_avg:1140.43ms
step:381/1390 train_time:423130ms step_avg:1140.51ms
step:382/1390 train_time:424404ms step_avg:1140.87ms
step:383/1390 train_time:425588ms step_avg:1140.99ms
step:384/1390 train_time:426790ms step_avg:1141.15ms
step:385/1390 train_time:428002ms step_avg:1141.34ms
step:386/1390 train_time:429210ms step_avg:1141.51ms
step:387/1390 train_time:430373ms step_avg:1141.57ms
step:388/1390 train_time:431555ms step_avg:1141.68ms
step:389/1390 train_time:432735ms step_avg:1141.78ms
step:390/1390 train_time:433949ms step_avg:1141.97ms
step:391/1390 train_time:435161ms step_avg:1142.15ms
step:392/1390 train_time:436343ms step_avg:1142.26ms
step:393/1390 train_time:437517ms step_avg:1142.34ms
step:394/1390 train_time:438699ms step_avg:1142.45ms
step:395/1390 train_time:439908ms step_avg:1142.62ms
step:396/1390 train_time:441108ms step_avg:1142.77ms
step:397/1390 train_time:442286ms step_avg:1142.86ms
step:398/1390 train_time:443469ms step_avg:1142.96ms
step:399/1390 train_time:444647ms step_avg:1143.05ms
step:400/1390 train_time:445859ms step_avg:1143.23ms
step:401/1390 train_time:447059ms step_avg:1143.37ms
step:402/1390 train_time:448267ms step_avg:1143.54ms
step:403/1390 train_time:449405ms step_avg:1143.52ms
step:404/1390 train_time:450600ms step_avg:1143.65ms
step:405/1390 train_time:451795ms step_avg:1143.78ms
step:406/1390 train_time:452985ms step_avg:1143.90ms
step:407/1390 train_time:454178ms step_avg:1144.03ms
step:408/1390 train_time:455344ms step_avg:1144.08ms
step:409/1390 train_time:456529ms step_avg:1144.18ms
step:410/1390 train_time:457727ms step_avg:1144.32ms
step:411/1390 train_time:458903ms step_avg:1144.40ms
step:412/1390 train_time:460091ms step_avg:1144.51ms
step:413/1390 train_time:461284ms step_avg:1144.63ms
step:414/1390 train_time:462515ms step_avg:1144.84ms
step:415/1390 train_time:463744ms step_avg:1145.05ms
step:416/1390 train_time:464974ms step_avg:1145.26ms
step:417/1390 train_time:466208ms step_avg:1145.47ms
step:418/1390 train_time:467408ms step_avg:1145.61ms
step:419/1390 train_time:468633ms step_avg:1145.80ms
step:420/1390 train_time:469859ms step_avg:1146.00ms
step:421/1390 train_time:471089ms step_avg:1146.20ms
step:422/1390 train_time:472312ms step_avg:1146.39ms
step:423/1390 train_time:473531ms step_avg:1146.56ms
step:424/1390 train_time:474752ms step_avg:1146.74ms
step:425/1390 train_time:475982ms step_avg:1146.94ms
step:426/1390 train_time:477193ms step_avg:1147.10ms
step:427/1390 train_time:478420ms step_avg:1147.29ms
step:428/1390 train_time:479652ms step_avg:1147.49ms
step:429/1390 train_time:480873ms step_avg:1147.67ms
step:430/1390 train_time:482078ms step_avg:1147.80ms
step:431/1390 train_time:483316ms step_avg:1148.02ms
step:432/1390 train_time:484538ms step_avg:1148.19ms
step:433/1390 train_time:485774ms step_avg:1148.40ms
step:434/1390 train_time:487006ms step_avg:1148.60ms
step:435/1390 train_time:488223ms step_avg:1148.76ms
step:436/1390 train_time:489421ms step_avg:1148.88ms
step:437/1390 train_time:490657ms step_avg:1149.08ms
step:438/1390 train_time:491889ms step_avg:1149.27ms
step:439/1390 train_time:493122ms step_avg:1149.47ms
step:440/1390 train_time:494359ms step_avg:1149.67ms
step:441/1390 train_time:495563ms step_avg:1149.80ms
step:442/1390 train_time:496792ms step_avg:1149.98ms
step:443/1390 train_time:498027ms step_avg:1150.18ms
step:444/1390 train_time:499255ms step_avg:1150.36ms
step:445/1390 train_time:500463ms step_avg:1150.49ms
step:446/1390 train_time:501696ms step_avg:1150.68ms
step:447/1390 train_time:502910ms step_avg:1150.82ms
step:448/1390 train_time:504166ms step_avg:1151.06ms
step:449/1390 train_time:505399ms step_avg:1151.25ms
step:450/1390 train_time:506622ms step_avg:1151.41ms
step:451/1390 train_time:507875ms step_avg:1151.64ms
step:452/1390 train_time:509106ms step_avg:1151.82ms
step:453/1390 train_time:510310ms step_avg:1151.94ms
step:454/1390 train_time:511562ms step_avg:1152.17ms
step:455/1390 train_time:512810ms step_avg:1152.38ms
step:456/1390 train_time:514023ms step_avg:1152.52ms
step:457/1390 train_time:515255ms step_avg:1152.70ms
step:458/1390 train_time:516488ms step_avg:1152.87ms
step:459/1390 train_time:517696ms step_avg:1153.00ms
step:460/1390 train_time:518941ms step_avg:1153.20ms
step:461/1390 train_time:520174ms step_avg:1153.38ms
step:462/1390 train_time:521418ms step_avg:1153.58ms
step:463/1390 train_time:522658ms step_avg:1153.77ms
step:464/1390 train_time:523873ms step_avg:1153.91ms
step:465/1390 train_time:525083ms step_avg:1154.03ms
step:466/1390 train_time:526310ms step_avg:1154.19ms
step:467/1390 train_time:527543ms step_avg:1154.36ms
step:468/1390 train_time:528772ms step_avg:1154.52ms
step:469/1390 train_time:529997ms step_avg:1154.68ms
step:470/1390 train_time:531229ms step_avg:1154.85ms
step:471/1390 train_time:532453ms step_avg:1155.00ms
step:472/1390 train_time:533710ms step_avg:1155.22ms
step:473/1390 train_time:534937ms step_avg:1155.37ms
step:474/1390 train_time:536146ms step_avg:1155.49ms
step:475/1390 train_time:537358ms step_avg:1155.61ms
step:476/1390 train_time:538582ms step_avg:1155.76ms
step:477/1390 train_time:539796ms step_avg:1155.88ms
step:478/1390 train_time:541044ms step_avg:1156.08ms
step:479/1390 train_time:542236ms step_avg:1156.15ms
step:480/1390 train_time:543482ms step_avg:1156.34ms
step:481/1390 train_time:544726ms step_avg:1156.53ms
step:482/1390 train_time:545968ms step_avg:1156.71ms
step:483/1390 train_time:547190ms step_avg:1156.85ms
step:484/1390 train_time:548434ms step_avg:1157.03ms
step:485/1390 train_time:549673ms step_avg:1157.21ms
step:486/1390 train_time:550904ms step_avg:1157.36ms
step:487/1390 train_time:552147ms step_avg:1157.54ms
step:488/1390 train_time:553378ms step_avg:1157.70ms
step:489/1390 train_time:554588ms step_avg:1157.80ms
step:490/1390 train_time:555835ms step_avg:1157.99ms
step:491/1390 train_time:557045ms step_avg:1158.10ms
step:492/1390 train_time:558280ms step_avg:1158.26ms
step:493/1390 train_time:559520ms step_avg:1158.43ms
step:494/1390 train_time:560724ms step_avg:1158.52ms
step:495/1390 train_time:561841ms step_avg:1158.43ms
step:496/1390 train_time:563157ms step_avg:1158.76ms
step:497/1390 train_time:564468ms step_avg:1159.07ms
step:498/1390 train_time:565642ms step_avg:1159.10ms
step:499/1390 train_time:566780ms step_avg:1159.06ms
step:500/1390 train_time:567904ms step_avg:1158.99ms
step:500/1390 val_loss:3.6538 train_time:568160ms step_avg:1159.51ms
step:501/1390 train_time:569042ms step_avg:1158.95ms
step:502/1390 train_time:570176ms step_avg:1158.89ms
step:503/1390 train_time:571348ms step_avg:1158.92ms
step:504/1390 train_time:572445ms step_avg:1158.79ms
step:505/1390 train_time:573581ms step_avg:1158.75ms
step:506/1390 train_time:574709ms step_avg:1158.69ms
step:507/1390 train_time:575853ms step_avg:1158.66ms
step:508/1390 train_time:577067ms step_avg:1158.77ms
step:509/1390 train_time:578295ms step_avg:1158.91ms
step:510/1390 train_time:579510ms step_avg:1159.02ms
step:511/1390 train_time:580730ms step_avg:1159.14ms
step:512/1390 train_time:581962ms step_avg:1159.29ms
step:513/1390 train_time:583194ms step_avg:1159.43ms
step:514/1390 train_time:584402ms step_avg:1159.53ms
step:515/1390 train_time:585659ms step_avg:1159.72ms
step:516/1390 train_time:586922ms step_avg:1159.92ms
step:517/1390 train_time:588191ms step_avg:1160.14ms
step:518/1390 train_time:589472ms step_avg:1160.38ms
step:519/1390 train_time:590736ms step_avg:1160.58ms
step:520/1390 train_time:592002ms step_avg:1160.79ms
step:521/1390 train_time:593251ms step_avg:1160.96ms
step:522/1390 train_time:594502ms step_avg:1161.14ms
step:523/1390 train_time:595786ms step_avg:1161.38ms
step:524/1390 train_time:597069ms step_avg:1161.61ms
step:525/1390 train_time:598362ms step_avg:1161.87ms
step:526/1390 train_time:599658ms step_avg:1162.13ms
step:527/1390 train_time:600903ms step_avg:1162.29ms
step:528/1390 train_time:602177ms step_avg:1162.50ms
step:529/1390 train_time:603423ms step_avg:1162.66ms
step:530/1390 train_time:604672ms step_avg:1162.83ms
step:531/1390 train_time:605951ms step_avg:1163.05ms
step:532/1390 train_time:607229ms step_avg:1163.27ms
step:533/1390 train_time:608481ms step_avg:1163.44ms
step:534/1390 train_time:609741ms step_avg:1163.63ms
step:535/1390 train_time:611017ms step_avg:1163.84ms
step:536/1390 train_time:612254ms step_avg:1163.98ms
step:537/1390 train_time:613515ms step_avg:1164.16ms
step:538/1390 train_time:614787ms step_avg:1164.37ms
step:539/1390 train_time:616067ms step_avg:1164.59ms
step:540/1390 train_time:617313ms step_avg:1164.74ms
step:541/1390 train_time:618594ms step_avg:1164.96ms
step:542/1390 train_time:619861ms step_avg:1165.15ms
step:543/1390 train_time:621115ms step_avg:1165.32ms
step:544/1390 train_time:622362ms step_avg:1165.47ms
step:545/1390 train_time:623653ms step_avg:1165.71ms
step:546/1390 train_time:624942ms step_avg:1165.94ms
step:547/1390 train_time:626190ms step_avg:1166.09ms
step:548/1390 train_time:627464ms step_avg:1166.29ms
step:549/1390 train_time:628748ms step_avg:1166.51ms
step:550/1390 train_time:629997ms step_avg:1166.66ms
step:551/1390 train_time:631262ms step_avg:1166.84ms
step:552/1390 train_time:632518ms step_avg:1167.01ms
step:553/1390 train_time:633779ms step_avg:1167.18ms
step:554/1390 train_time:635020ms step_avg:1167.32ms
step:555/1390 train_time:636279ms step_avg:1167.49ms
step:556/1390 train_time:637543ms step_avg:1167.66ms
step:557/1390 train_time:638786ms step_avg:1167.80ms
step:558/1390 train_time:640003ms step_avg:1167.89ms
step:559/1390 train_time:641157ms step_avg:1167.86ms
step:560/1390 train_time:642432ms step_avg:1168.06ms
step:561/1390 train_time:643783ms step_avg:1168.39ms
step:562/1390 train_time:645059ms step_avg:1168.58ms
step:563/1390 train_time:646217ms step_avg:1168.57ms
step:564/1390 train_time:647363ms step_avg:1168.53ms
step:565/1390 train_time:648536ms step_avg:1168.53ms
step:566/1390 train_time:649692ms step_avg:1168.51ms
step:567/1390 train_time:650883ms step_avg:1168.55ms
step:568/1390 train_time:652050ms step_avg:1168.55ms
step:569/1390 train_time:653228ms step_avg:1168.57ms
step:570/1390 train_time:654396ms step_avg:1168.56ms
step:571/1390 train_time:655543ms step_avg:1168.53ms
step:572/1390 train_time:656698ms step_avg:1168.50ms
step:573/1390 train_time:657839ms step_avg:1168.45ms
step:574/1390 train_time:659018ms step_avg:1168.47ms
step:575/1390 train_time:660188ms step_avg:1168.47ms
step:576/1390 train_time:661347ms step_avg:1168.46ms
step:577/1390 train_time:662512ms step_avg:1168.45ms
step:578/1390 train_time:663685ms step_avg:1168.46ms
step:579/1390 train_time:664846ms step_avg:1168.45ms
step:580/1390 train_time:666022ms step_avg:1168.46ms
step:581/1390 train_time:667224ms step_avg:1168.52ms
step:582/1390 train_time:668463ms step_avg:1168.64ms
step:583/1390 train_time:669631ms step_avg:1168.64ms
step:584/1390 train_time:670884ms step_avg:1168.79ms
step:585/1390 train_time:672174ms step_avg:1169.00ms
step:586/1390 train_time:673399ms step_avg:1169.10ms
step:587/1390 train_time:674670ms step_avg:1169.27ms
step:588/1390 train_time:675940ms step_avg:1169.45ms
step:589/1390 train_time:677218ms step_avg:1169.63ms
step:590/1390 train_time:678477ms step_avg:1169.79ms
step:591/1390 train_time:679735ms step_avg:1169.94ms
step:592/1390 train_time:680989ms step_avg:1170.08ms
step:593/1390 train_time:682251ms step_avg:1170.24ms
step:594/1390 train_time:683506ms step_avg:1170.39ms
step:595/1390 train_time:684759ms step_avg:1170.53ms
step:596/1390 train_time:686049ms step_avg:1170.73ms
step:597/1390 train_time:687316ms step_avg:1170.90ms
step:598/1390 train_time:688560ms step_avg:1171.02ms
step:599/1390 train_time:689824ms step_avg:1171.18ms
step:600/1390 train_time:691080ms step_avg:1171.32ms
step:601/1390 train_time:692332ms step_avg:1171.46ms
step:602/1390 train_time:693611ms step_avg:1171.64ms
step:603/1390 train_time:694870ms step_avg:1171.79ms
step:604/1390 train_time:696143ms step_avg:1171.96ms
step:605/1390 train_time:697435ms step_avg:1172.16ms
step:606/1390 train_time:698686ms step_avg:1172.29ms
step:607/1390 train_time:699949ms step_avg:1172.44ms
step:608/1390 train_time:701211ms step_avg:1172.59ms
step:609/1390 train_time:702453ms step_avg:1172.71ms
step:610/1390 train_time:703718ms step_avg:1172.86ms
step:611/1390 train_time:704981ms step_avg:1173.01ms
step:612/1390 train_time:706227ms step_avg:1173.13ms
step:613/1390 train_time:707495ms step_avg:1173.29ms
step:614/1390 train_time:708738ms step_avg:1173.41ms
step:615/1390 train_time:710010ms step_avg:1173.57ms
step:616/1390 train_time:711290ms step_avg:1173.75ms
step:617/1390 train_time:712549ms step_avg:1173.89ms
step:618/1390 train_time:713836ms step_avg:1174.07ms
step:619/1390 train_time:715098ms step_avg:1174.22ms
step:620/1390 train_time:716389ms step_avg:1174.41ms
step:621/1390 train_time:717692ms step_avg:1174.62ms
step:622/1390 train_time:718988ms step_avg:1174.82ms
step:623/1390 train_time:720292ms step_avg:1175.03ms
step:624/1390 train_time:721600ms step_avg:1175.24ms
step:625/1390 train_time:722920ms step_avg:1175.48ms
step:625/1390 val_loss:3.5716 train_time:723212ms step_avg:1175.95ms
step:626/1390 train_time:724225ms step_avg:1175.69ms
step:627/1390 train_time:725552ms step_avg:1175.94ms
step:628/1390 train_time:726834ms step_avg:1176.11ms
step:629/1390 train_time:728112ms step_avg:1176.27ms
step:630/1390 train_time:729416ms step_avg:1176.48ms
step:631/1390 train_time:730711ms step_avg:1176.67ms
step:632/1390 train_time:732002ms step_avg:1176.85ms
step:633/1390 train_time:733275ms step_avg:1177.01ms
step:634/1390 train_time:734540ms step_avg:1177.15ms
step:635/1390 train_time:735824ms step_avg:1177.32ms
step:636/1390 train_time:737135ms step_avg:1177.53ms
step:637/1390 train_time:738452ms step_avg:1177.76ms
step:638/1390 train_time:739774ms step_avg:1177.98ms
step:639/1390 train_time:741075ms step_avg:1178.18ms
step:640/1390 train_time:742361ms step_avg:1178.35ms
step:641/1390 train_time:743684ms step_avg:1178.58ms
step:642/1390 train_time:744971ms step_avg:1178.75ms
step:643/1390 train_time:746226ms step_avg:1178.87ms
step:644/1390 train_time:747519ms step_avg:1179.05ms
step:645/1390 train_time:748821ms step_avg:1179.25ms
step:646/1390 train_time:750130ms step_avg:1179.45ms
step:647/1390 train_time:751432ms step_avg:1179.64ms
step:648/1390 train_time:752728ms step_avg:1179.82ms
step:649/1390 train_time:754048ms step_avg:1180.04ms
step:650/1390 train_time:755351ms step_avg:1180.24ms
step:651/1390 train_time:756632ms step_avg:1180.39ms
step:652/1390 train_time:757907ms step_avg:1180.54ms
step:653/1390 train_time:759210ms step_avg:1180.73ms
step:654/1390 train_time:760515ms step_avg:1180.92ms
step:655/1390 train_time:761827ms step_avg:1181.13ms
step:656/1390 train_time:763117ms step_avg:1181.30ms
step:657/1390 train_time:764399ms step_avg:1181.45ms
step:658/1390 train_time:765683ms step_avg:1181.61ms
step:659/1390 train_time:766955ms step_avg:1181.75ms
step:660/1390 train_time:768238ms step_avg:1181.91ms
step:661/1390 train_time:769527ms step_avg:1182.07ms
step:662/1390 train_time:770837ms step_avg:1182.26ms
step:663/1390 train_time:772127ms step_avg:1182.43ms
step:664/1390 train_time:773406ms step_avg:1182.58ms
step:665/1390 train_time:774697ms step_avg:1182.74ms
step:666/1390 train_time:776006ms step_avg:1182.94ms
step:667/1390 train_time:777286ms step_avg:1183.08ms
step:668/1390 train_time:778552ms step_avg:1183.21ms
step:669/1390 train_time:779838ms step_avg:1183.37ms
step:670/1390 train_time:781137ms step_avg:1183.54ms
step:671/1390 train_time:782439ms step_avg:1183.72ms
step:672/1390 train_time:783732ms step_avg:1183.88ms
step:673/1390 train_time:785060ms step_avg:1184.10ms
step:674/1390 train_time:786358ms step_avg:1184.27ms
step:675/1390 train_time:787634ms step_avg:1184.41ms
step:676/1390 train_time:788952ms step_avg:1184.61ms
step:677/1390 train_time:790235ms step_avg:1184.76ms
step:678/1390 train_time:791518ms step_avg:1184.91ms
step:679/1390 train_time:792805ms step_avg:1185.06ms
step:680/1390 train_time:794099ms step_avg:1185.22ms
step:681/1390 train_time:795398ms step_avg:1185.39ms
step:682/1390 train_time:796659ms step_avg:1185.50ms
step:683/1390 train_time:797928ms step_avg:1185.63ms
step:684/1390 train_time:799222ms step_avg:1185.79ms
step:685/1390 train_time:800515ms step_avg:1185.95ms
step:686/1390 train_time:801844ms step_avg:1186.16ms
step:687/1390 train_time:803130ms step_avg:1186.31ms
step:688/1390 train_time:804398ms step_avg:1186.43ms
step:689/1390 train_time:805707ms step_avg:1186.61ms
step:690/1390 train_time:807017ms step_avg:1186.79ms
step:691/1390 train_time:808339ms step_avg:1186.99ms
step:692/1390 train_time:809609ms step_avg:1187.11ms
step:693/1390 train_time:810882ms step_avg:1187.24ms
step:694/1390 train_time:812168ms step_avg:1187.38ms
step:695/1390 train_time:813431ms step_avg:1187.49ms
step:696/1390 train_time:814699ms step_avg:1187.61ms
step:697/1390 train_time:815985ms step_avg:1187.75ms
step:698/1390 train_time:817262ms step_avg:1187.88ms
step:699/1390 train_time:818550ms step_avg:1188.03ms
step:700/1390 train_time:819861ms step_avg:1188.20ms
step:701/1390 train_time:821140ms step_avg:1188.34ms
step:702/1390 train_time:822418ms step_avg:1188.47ms
step:703/1390 train_time:823709ms step_avg:1188.61ms
step:704/1390 train_time:825025ms step_avg:1188.80ms
step:705/1390 train_time:826318ms step_avg:1188.95ms
step:706/1390 train_time:827603ms step_avg:1189.08ms
step:707/1390 train_time:828933ms step_avg:1189.29ms
step:708/1390 train_time:830229ms step_avg:1189.44ms
step:709/1390 train_time:831519ms step_avg:1189.58ms
step:710/1390 train_time:832798ms step_avg:1189.71ms
step:711/1390 train_time:834074ms step_avg:1189.83ms
step:712/1390 train_time:835394ms step_avg:1190.02ms
step:713/1390 train_time:836678ms step_avg:1190.15ms
step:714/1390 train_time:837985ms step_avg:1190.32ms
step:715/1390 train_time:839264ms step_avg:1190.44ms
step:716/1390 train_time:840559ms step_avg:1190.59ms
step:717/1390 train_time:841877ms step_avg:1190.77ms
step:718/1390 train_time:843169ms step_avg:1190.92ms
step:719/1390 train_time:844460ms step_avg:1191.06ms
step:720/1390 train_time:845721ms step_avg:1191.16ms
step:721/1390 train_time:847018ms step_avg:1191.31ms
step:722/1390 train_time:848291ms step_avg:1191.42ms
step:723/1390 train_time:849618ms step_avg:1191.61ms
step:724/1390 train_time:850931ms step_avg:1191.78ms
step:725/1390 train_time:852282ms step_avg:1192.00ms
step:726/1390 train_time:853592ms step_avg:1192.17ms
step:727/1390 train_time:854922ms step_avg:1192.36ms
step:728/1390 train_time:856305ms step_avg:1192.63ms
step:729/1390 train_time:857616ms step_avg:1192.79ms
step:730/1390 train_time:858913ms step_avg:1192.93ms
step:731/1390 train_time:860247ms step_avg:1193.13ms
step:732/1390 train_time:861576ms step_avg:1193.32ms
step:733/1390 train_time:862883ms step_avg:1193.48ms
step:734/1390 train_time:864228ms step_avg:1193.68ms
step:735/1390 train_time:865550ms step_avg:1193.86ms
step:736/1390 train_time:866875ms step_avg:1194.04ms
step:737/1390 train_time:868236ms step_avg:1194.27ms
step:738/1390 train_time:869600ms step_avg:1194.51ms
step:739/1390 train_time:870927ms step_avg:1194.69ms
step:740/1390 train_time:872249ms step_avg:1194.86ms
step:741/1390 train_time:873581ms step_avg:1195.05ms
step:742/1390 train_time:874831ms step_avg:1195.12ms
step:743/1390 train_time:876141ms step_avg:1195.28ms
step:744/1390 train_time:877489ms step_avg:1195.49ms
step:745/1390 train_time:878810ms step_avg:1195.66ms
step:746/1390 train_time:880211ms step_avg:1195.94ms
step:747/1390 train_time:881493ms step_avg:1196.06ms
step:748/1390 train_time:882799ms step_avg:1196.21ms
step:749/1390 train_time:884094ms step_avg:1196.34ms
step:750/1390 train_time:885392ms step_avg:1196.48ms
step:750/1390 val_loss:3.5205 train_time:885712ms step_avg:1196.91ms
step:751/1390 train_time:886681ms step_avg:1196.60ms
step:752/1390 train_time:888077ms step_avg:1196.87ms
step:753/1390 train_time:889398ms step_avg:1197.04ms
step:754/1390 train_time:890720ms step_avg:1197.20ms
step:755/1390 train_time:892025ms step_avg:1197.35ms
step:756/1390 train_time:893348ms step_avg:1197.52ms
step:757/1390 train_time:894618ms step_avg:1197.61ms
step:758/1390 train_time:895960ms step_avg:1197.81ms
step:759/1390 train_time:897286ms step_avg:1197.98ms
step:760/1390 train_time:898636ms step_avg:1198.18ms
step:761/1390 train_time:899932ms step_avg:1198.31ms
step:762/1390 train_time:901262ms step_avg:1198.49ms
step:763/1390 train_time:902634ms step_avg:1198.72ms
step:764/1390 train_time:903936ms step_avg:1198.85ms
step:765/1390 train_time:905260ms step_avg:1199.02ms
step:766/1390 train_time:906628ms step_avg:1199.24ms
step:767/1390 train_time:907972ms step_avg:1199.43ms
step:768/1390 train_time:909285ms step_avg:1199.58ms
step:769/1390 train_time:910635ms step_avg:1199.78ms
step:770/1390 train_time:911938ms step_avg:1199.92ms
step:771/1390 train_time:913299ms step_avg:1200.13ms
step:772/1390 train_time:914593ms step_avg:1200.25ms
step:773/1390 train_time:915907ms step_avg:1200.40ms
step:774/1390 train_time:917261ms step_avg:1200.60ms
step:775/1390 train_time:918614ms step_avg:1200.80ms
step:776/1390 train_time:919961ms step_avg:1200.99ms
step:777/1390 train_time:921258ms step_avg:1201.12ms
step:778/1390 train_time:922531ms step_avg:1201.21ms
step:779/1390 train_time:923804ms step_avg:1201.31ms
step:780/1390 train_time:925084ms step_avg:1201.41ms
step:781/1390 train_time:926436ms step_avg:1201.60ms
step:782/1390 train_time:927755ms step_avg:1201.76ms
step:783/1390 train_time:929094ms step_avg:1201.93ms
step:784/1390 train_time:930416ms step_avg:1202.09ms
step:785/1390 train_time:931752ms step_avg:1202.26ms
step:786/1390 train_time:933121ms step_avg:1202.48ms
step:787/1390 train_time:934417ms step_avg:1202.60ms
step:788/1390 train_time:935764ms step_avg:1202.78ms
step:789/1390 train_time:937079ms step_avg:1202.92ms
step:790/1390 train_time:938391ms step_avg:1203.07ms
step:791/1390 train_time:939713ms step_avg:1203.22ms
step:792/1390 train_time:941042ms step_avg:1203.38ms
step:793/1390 train_time:942332ms step_avg:1203.49ms
step:794/1390 train_time:943685ms step_avg:1203.68ms
step:795/1390 train_time:945036ms step_avg:1203.87ms
step:796/1390 train_time:946380ms step_avg:1204.05ms
step:797/1390 train_time:947764ms step_avg:1204.27ms
step:798/1390 train_time:949093ms step_avg:1204.43ms
step:799/1390 train_time:950417ms step_avg:1204.58ms
step:800/1390 train_time:951760ms step_avg:1204.76ms
step:801/1390 train_time:953052ms step_avg:1204.87ms
step:802/1390 train_time:954397ms step_avg:1205.05ms
step:803/1390 train_time:955713ms step_avg:1205.19ms
step:804/1390 train_time:957021ms step_avg:1205.32ms
step:805/1390 train_time:958331ms step_avg:1205.45ms
step:806/1390 train_time:959739ms step_avg:1205.70ms
step:807/1390 train_time:961018ms step_avg:1205.79ms
step:808/1390 train_time:962314ms step_avg:1205.91ms
step:809/1390 train_time:963597ms step_avg:1206.00ms
step:810/1390 train_time:964859ms step_avg:1206.07ms
step:811/1390 train_time:966232ms step_avg:1206.28ms
step:812/1390 train_time:967520ms step_avg:1206.38ms
step:813/1390 train_time:968821ms step_avg:1206.50ms
step:814/1390 train_time:970150ms step_avg:1206.65ms
step:815/1390 train_time:971464ms step_avg:1206.79ms
step:816/1390 train_time:972786ms step_avg:1206.93ms
step:817/1390 train_time:974148ms step_avg:1207.12ms
step:818/1390 train_time:975441ms step_avg:1207.23ms
step:819/1390 train_time:976713ms step_avg:1207.31ms
step:820/1390 train_time:978063ms step_avg:1207.49ms
step:821/1390 train_time:979396ms step_avg:1207.64ms
step:822/1390 train_time:980714ms step_avg:1207.78ms
step:823/1390 train_time:982031ms step_avg:1207.91ms
step:824/1390 train_time:983345ms step_avg:1208.04ms
step:825/1390 train_time:984689ms step_avg:1208.21ms
step:826/1390 train_time:986059ms step_avg:1208.41ms
step:827/1390 train_time:987392ms step_avg:1208.56ms
step:828/1390 train_time:988720ms step_avg:1208.70ms
step:829/1390 train_time:990127ms step_avg:1208.95ms
step:830/1390 train_time:991437ms step_avg:1209.07ms
step:831/1390 train_time:992803ms step_avg:1209.26ms
step:832/1390 train_time:994151ms step_avg:1209.43ms
step:833/1390 train_time:995492ms step_avg:1209.59ms
step:834/1390 train_time:996809ms step_avg:1209.72ms
step:835/1390 train_time:998144ms step_avg:1209.87ms
step:836/1390 train_time:999533ms step_avg:1210.09ms
step:837/1390 train_time:1000938ms step_avg:1210.32ms
step:838/1390 train_time:1002208ms step_avg:1210.40ms
step:839/1390 train_time:1003574ms step_avg:1210.58ms
step:840/1390 train_time:1004852ms step_avg:1210.67ms
step:841/1390 train_time:1006252ms step_avg:1210.89ms
step:842/1390 train_time:1007580ms step_avg:1211.03ms
step:843/1390 train_time:1008943ms step_avg:1211.22ms
step:844/1390 train_time:1010234ms step_avg:1211.31ms
step:845/1390 train_time:1011595ms step_avg:1211.49ms
step:846/1390 train_time:1012956ms step_avg:1211.67ms
step:847/1390 train_time:1014296ms step_avg:1211.82ms
step:848/1390 train_time:1015709ms step_avg:1212.06ms
step:849/1390 train_time:1017041ms step_avg:1212.21ms
step:850/1390 train_time:1018385ms step_avg:1212.36ms
step:851/1390 train_time:1019761ms step_avg:1212.56ms
step:852/1390 train_time:1021093ms step_avg:1212.70ms
step:853/1390 train_time:1022420ms step_avg:1212.84ms
step:854/1390 train_time:1023781ms step_avg:1213.01ms
step:855/1390 train_time:1025124ms step_avg:1213.16ms
step:856/1390 train_time:1026433ms step_avg:1213.28ms
step:857/1390 train_time:1027728ms step_avg:1213.37ms
step:858/1390 train_time:1029084ms step_avg:1213.54ms
step:859/1390 train_time:1030451ms step_avg:1213.72ms
step:860/1390 train_time:1031812ms step_avg:1213.90ms
step:861/1390 train_time:1033153ms step_avg:1214.05ms
step:862/1390 train_time:1034455ms step_avg:1214.15ms
step:863/1390 train_time:1035795ms step_avg:1214.30ms
step:864/1390 train_time:1037138ms step_avg:1214.45ms
step:865/1390 train_time:1038521ms step_avg:1214.64ms
step:866/1390 train_time:1039871ms step_avg:1214.80ms
step:867/1390 train_time:1041207ms step_avg:1214.94ms
step:868/1390 train_time:1042687ms step_avg:1215.25ms
step:869/1390 train_time:1044050ms step_avg:1215.42ms
step:870/1390 train_time:1045385ms step_avg:1215.56ms
step:871/1390 train_time:1046801ms step_avg:1215.80ms
step:872/1390 train_time:1048117ms step_avg:1215.91ms
step:873/1390 train_time:1049454ms step_avg:1216.05ms
step:874/1390 train_time:1050766ms step_avg:1216.16ms
step:875/1390 train_time:1052125ms step_avg:1216.33ms
step:875/1390 val_loss:3.4718 train_time:1052484ms step_avg:1216.74ms
step:876/1390 train_time:1053457ms step_avg:1216.46ms
step:877/1390 train_time:1054810ms step_avg:1216.62ms
step:878/1390 train_time:1056198ms step_avg:1216.82ms
step:879/1390 train_time:1057522ms step_avg:1216.94ms
step:880/1390 train_time:1058937ms step_avg:1217.17ms
step:881/1390 train_time:1060304ms step_avg:1217.34ms
step:882/1390 train_time:1061620ms step_avg:1217.45ms
step:883/1390 train_time:1062962ms step_avg:1217.60ms
step:884/1390 train_time:1064266ms step_avg:1217.70ms
step:885/1390 train_time:1065591ms step_avg:1217.82ms
step:886/1390 train_time:1066916ms step_avg:1217.94ms
step:887/1390 train_time:1068253ms step_avg:1218.08ms
step:888/1390 train_time:1069546ms step_avg:1218.16ms
step:889/1390 train_time:1070894ms step_avg:1218.31ms
step:890/1390 train_time:1072243ms step_avg:1218.46ms
step:891/1390 train_time:1073563ms step_avg:1218.57ms
step:892/1390 train_time:1074915ms step_avg:1218.72ms
step:893/1390 train_time:1076274ms step_avg:1218.88ms
step:894/1390 train_time:1077589ms step_avg:1218.99ms
step:895/1390 train_time:1078923ms step_avg:1219.12ms
step:896/1390 train_time:1080272ms step_avg:1219.27ms
step:897/1390 train_time:1081553ms step_avg:1219.34ms
step:898/1390 train_time:1082892ms step_avg:1219.47ms
step:899/1390 train_time:1084204ms step_avg:1219.58ms
step:900/1390 train_time:1085543ms step_avg:1219.71ms
step:901/1390 train_time:1086859ms step_avg:1219.82ms
step:902/1390 train_time:1088197ms step_avg:1219.95ms
step:903/1390 train_time:1089530ms step_avg:1220.08ms
step:904/1390 train_time:1090854ms step_avg:1220.19ms
step:905/1390 train_time:1092240ms step_avg:1220.38ms
step:906/1390 train_time:1093594ms step_avg:1220.53ms
step:907/1390 train_time:1094891ms step_avg:1220.61ms
step:908/1390 train_time:1096257ms step_avg:1220.78ms
step:909/1390 train_time:1097538ms step_avg:1220.84ms
step:910/1390 train_time:1098897ms step_avg:1221.00ms
step:911/1390 train_time:1100288ms step_avg:1221.19ms
step:912/1390 train_time:1101551ms step_avg:1221.23ms
step:913/1390 train_time:1102909ms step_avg:1221.38ms
step:914/1390 train_time:1104252ms step_avg:1221.52ms
step:915/1390 train_time:1105540ms step_avg:1221.59ms
step:916/1390 train_time:1106870ms step_avg:1221.71ms
step:917/1390 train_time:1108212ms step_avg:1221.84ms
step:918/1390 train_time:1109579ms step_avg:1222.00ms
step:919/1390 train_time:1110930ms step_avg:1222.15ms
step:920/1390 train_time:1112227ms step_avg:1222.23ms
step:921/1390 train_time:1113579ms step_avg:1222.37ms
step:922/1390 train_time:1114951ms step_avg:1222.53ms
step:923/1390 train_time:1116331ms step_avg:1222.71ms
step:924/1390 train_time:1117656ms step_avg:1222.82ms
step:925/1390 train_time:1118984ms step_avg:1222.93ms
step:926/1390 train_time:1120370ms step_avg:1223.11ms
step:927/1390 train_time:1121695ms step_avg:1223.22ms
step:928/1390 train_time:1123010ms step_avg:1223.32ms
step:929/1390 train_time:1124354ms step_avg:1223.45ms
step:930/1390 train_time:1125795ms step_avg:1223.69ms
step:931/1390 train_time:1127149ms step_avg:1223.83ms
step:932/1390 train_time:1128459ms step_avg:1223.92ms
step:933/1390 train_time:1129769ms step_avg:1224.02ms
step:934/1390 train_time:1131156ms step_avg:1224.19ms
step:935/1390 train_time:1132512ms step_avg:1224.34ms
step:936/1390 train_time:1133861ms step_avg:1224.47ms
step:937/1390 train_time:1135221ms step_avg:1224.62ms
step:938/1390 train_time:1136612ms step_avg:1224.80ms
step:939/1390 train_time:1137966ms step_avg:1224.94ms
step:940/1390 train_time:1139300ms step_avg:1225.05ms
step:941/1390 train_time:1140653ms step_avg:1225.19ms
step:942/1390 train_time:1142046ms step_avg:1225.37ms
step:943/1390 train_time:1143365ms step_avg:1225.47ms
step:944/1390 train_time:1144734ms step_avg:1225.63ms
step:945/1390 train_time:1146117ms step_avg:1225.79ms
step:946/1390 train_time:1147491ms step_avg:1225.95ms
step:947/1390 train_time:1148831ms step_avg:1226.07ms
step:948/1390 train_time:1150182ms step_avg:1226.21ms
step:949/1390 train_time:1151549ms step_avg:1226.36ms
step:950/1390 train_time:1152964ms step_avg:1226.56ms
step:951/1390 train_time:1154294ms step_avg:1226.67ms
step:952/1390 train_time:1155692ms step_avg:1226.85ms
step:953/1390 train_time:1157013ms step_avg:1226.95ms
step:954/1390 train_time:1158369ms step_avg:1227.09ms
step:955/1390 train_time:1159707ms step_avg:1227.20ms
step:956/1390 train_time:1161071ms step_avg:1227.35ms
step:957/1390 train_time:1162390ms step_avg:1227.44ms
step:958/1390 train_time:1163776ms step_avg:1227.61ms
step:959/1390 train_time:1165122ms step_avg:1227.74ms
step:960/1390 train_time:1166460ms step_avg:1227.85ms
step:961/1390 train_time:1167904ms step_avg:1228.08ms
step:962/1390 train_time:1169245ms step_avg:1228.20ms
step:963/1390 train_time:1170557ms step_avg:1228.29ms
step:964/1390 train_time:1171903ms step_avg:1228.41ms
step:965/1390 train_time:1173391ms step_avg:1228.68ms
step:966/1390 train_time:1174723ms step_avg:1228.79ms
step:967/1390 train_time:1176043ms step_avg:1228.89ms
step:968/1390 train_time:1177391ms step_avg:1229.01ms
step:969/1390 train_time:1178769ms step_avg:1229.16ms
step:970/1390 train_time:1180112ms step_avg:1229.28ms
step:971/1390 train_time:1181495ms step_avg:1229.44ms
step:972/1390 train_time:1182852ms step_avg:1229.58ms
step:973/1390 train_time:1184197ms step_avg:1229.70ms
step:974/1390 train_time:1185554ms step_avg:1229.83ms
step:975/1390 train_time:1186916ms step_avg:1229.96ms
step:976/1390 train_time:1188268ms step_avg:1230.09ms
step:977/1390 train_time:1189633ms step_avg:1230.23ms
step:978/1390 train_time:1190973ms step_avg:1230.34ms
step:979/1390 train_time:1192326ms step_avg:1230.47ms
step:980/1390 train_time:1193670ms step_avg:1230.59ms
step:981/1390 train_time:1194982ms step_avg:1230.67ms
step:982/1390 train_time:1196308ms step_avg:1230.77ms
step:983/1390 train_time:1197660ms step_avg:1230.89ms
step:984/1390 train_time:1198998ms step_avg:1231.00ms
step:985/1390 train_time:1200329ms step_avg:1231.11ms
step:986/1390 train_time:1201639ms step_avg:1231.19ms
step:987/1390 train_time:1203034ms step_avg:1231.36ms
step:988/1390 train_time:1204439ms step_avg:1231.53ms
step:989/1390 train_time:1205767ms step_avg:1231.63ms
step:990/1390 train_time:1207143ms step_avg:1231.78ms
step:991/1390 train_time:1208486ms step_avg:1231.89ms
step:992/1390 train_time:1209905ms step_avg:1232.08ms
step:993/1390 train_time:1211244ms step_avg:1232.19ms
step:994/1390 train_time:1212691ms step_avg:1232.41ms
step:995/1390 train_time:1214105ms step_avg:1232.59ms
step:996/1390 train_time:1215479ms step_avg:1232.74ms
step:997/1390 train_time:1216834ms step_avg:1232.86ms
step:998/1390 train_time:1218187ms step_avg:1232.98ms
step:999/1390 train_time:1219529ms step_avg:1233.09ms
step:1000/1390 train_time:1220849ms step_avg:1233.18ms
step:1000/1390 val_loss:3.4076 train_time:1221223ms step_avg:1233.56ms
step:1001/1390 train_time:1222245ms step_avg:1233.35ms
step:1002/1390 train_time:1223597ms step_avg:1233.46ms
step:1003/1390 train_time:1224951ms step_avg:1233.59ms
step:1004/1390 train_time:1226312ms step_avg:1233.71ms
step:1005/1390 train_time:1227667ms step_avg:1233.84ms
step:1006/1390 train_time:1229035ms step_avg:1233.97ms
step:1007/1390 train_time:1230377ms step_avg:1234.08ms
step:1008/1390 train_time:1231727ms step_avg:1234.20ms
step:1009/1390 train_time:1233061ms step_avg:1234.30ms
step:1010/1390 train_time:1234446ms step_avg:1234.45ms
step:1011/1390 train_time:1235825ms step_avg:1234.59ms
step:1012/1390 train_time:1237175ms step_avg:1234.71ms
step:1013/1390 train_time:1238564ms step_avg:1234.86ms
step:1014/1390 train_time:1239917ms step_avg:1234.98ms
step:1015/1390 train_time:1241305ms step_avg:1235.13ms
step:1016/1390 train_time:1242688ms step_avg:1235.28ms
step:1017/1390 train_time:1244070ms step_avg:1235.42ms
step:1018/1390 train_time:1245412ms step_avg:1235.53ms
step:1019/1390 train_time:1246759ms step_avg:1235.64ms
step:1020/1390 train_time:1248076ms step_avg:1235.72ms
step:1021/1390 train_time:1249509ms step_avg:1235.91ms
step:1022/1390 train_time:1250941ms step_avg:1236.11ms
step:1023/1390 train_time:1252213ms step_avg:1236.14ms
step:1024/1390 train_time:1253521ms step_avg:1236.21ms
step:1025/1390 train_time:1254892ms step_avg:1236.35ms
step:1026/1390 train_time:1256267ms step_avg:1236.48ms
step:1027/1390 train_time:1257647ms step_avg:1236.62ms
step:1028/1390 train_time:1258947ms step_avg:1236.69ms
step:1029/1390 train_time:1260301ms step_avg:1236.80ms
step:1030/1390 train_time:1261721ms step_avg:1236.98ms
step:1031/1390 train_time:1263130ms step_avg:1237.15ms
step:1032/1390 train_time:1264523ms step_avg:1237.30ms
step:1033/1390 train_time:1265842ms step_avg:1237.38ms
step:1034/1390 train_time:1267246ms step_avg:1237.54ms
step:1035/1390 train_time:1268612ms step_avg:1237.67ms
step:1036/1390 train_time:1269995ms step_avg:1237.81ms
step:1037/1390 train_time:1271412ms step_avg:1237.99ms
step:1038/1390 train_time:1272781ms step_avg:1238.11ms
step:1039/1390 train_time:1274245ms step_avg:1238.33ms
step:1040/1390 train_time:1275594ms step_avg:1238.44ms
step:1041/1390 train_time:1276996ms step_avg:1238.60ms
step:1042/1390 train_time:1278378ms step_avg:1238.74ms
step:1043/1390 train_time:1279769ms step_avg:1238.89ms
step:1044/1390 train_time:1281117ms step_avg:1238.99ms
step:1045/1390 train_time:1282471ms step_avg:1239.10ms
step:1046/1390 train_time:1283865ms step_avg:1239.25ms
step:1047/1390 train_time:1285300ms step_avg:1239.44ms
step:1048/1390 train_time:1286646ms step_avg:1239.54ms
step:1049/1390 train_time:1288002ms step_avg:1239.66ms
step:1050/1390 train_time:1289396ms step_avg:1239.80ms
step:1051/1390 train_time:1290754ms step_avg:1239.92ms
step:1052/1390 train_time:1292189ms step_avg:1240.10ms
step:1053/1390 train_time:1293622ms step_avg:1240.29ms
step:1054/1390 train_time:1294997ms step_avg:1240.42ms
step:1055/1390 train_time:1296343ms step_avg:1240.52ms
step:1056/1390 train_time:1297727ms step_avg:1240.66ms
step:1057/1390 train_time:1299113ms step_avg:1240.80ms
step:1058/1390 train_time:1300460ms step_avg:1240.90ms
step:1059/1390 train_time:1301837ms step_avg:1241.03ms
step:1060/1390 train_time:1303270ms step_avg:1241.21ms
step:1061/1390 train_time:1304683ms step_avg:1241.37ms
step:1062/1390 train_time:1305971ms step_avg:1241.42ms
step:1063/1390 train_time:1307340ms step_avg:1241.54ms
step:1064/1390 train_time:1308746ms step_avg:1241.69ms
step:1065/1390 train_time:1310069ms step_avg:1241.77ms
step:1066/1390 train_time:1311459ms step_avg:1241.91ms
step:1067/1390 train_time:1312865ms step_avg:1242.07ms
step:1068/1390 train_time:1314211ms step_avg:1242.17ms
step:1069/1390 train_time:1315608ms step_avg:1242.31ms
step:1070/1390 train_time:1316932ms step_avg:1242.39ms
step:1071/1390 train_time:1318344ms step_avg:1242.55ms
step:1072/1390 train_time:1319695ms step_avg:1242.65ms
step:1073/1390 train_time:1321092ms step_avg:1242.80ms
step:1074/1390 train_time:1322456ms step_avg:1242.91ms
step:1075/1390 train_time:1323822ms step_avg:1243.03ms
step:1076/1390 train_time:1325199ms step_avg:1243.15ms
step:1077/1390 train_time:1326516ms step_avg:1243.22ms
step:1078/1390 train_time:1327896ms step_avg:1243.35ms
step:1079/1390 train_time:1329210ms step_avg:1243.41ms
step:1080/1390 train_time:1330666ms step_avg:1243.61ms
step:1081/1390 train_time:1331899ms step_avg:1243.60ms
step:1082/1390 train_time:1333331ms step_avg:1243.78ms
step:1083/1390 train_time:1334628ms step_avg:1243.83ms
step:1084/1390 train_time:1336030ms step_avg:1243.98ms
step:1085/1390 train_time:1337399ms step_avg:1244.09ms
step:1086/1390 train_time:1338901ms step_avg:1244.33ms
step:1087/1390 train_time:1340226ms step_avg:1244.41ms
step:1088/1390 train_time:1341639ms step_avg:1244.56ms
step:1089/1390 train_time:1342997ms step_avg:1244.67ms
step:1090/1390 train_time:1344392ms step_avg:1244.81ms
step:1091/1390 train_time:1345833ms step_avg:1244.99ms
step:1092/1390 train_time:1347339ms step_avg:1245.23ms
step:1093/1390 train_time:1348666ms step_avg:1245.31ms
step:1094/1390 train_time:1350050ms step_avg:1245.43ms
step:1095/1390 train_time:1351472ms step_avg:1245.60ms
step:1096/1390 train_time:1352820ms step_avg:1245.69ms
step:1097/1390 train_time:1354163ms step_avg:1245.78ms
step:1098/1390 train_time:1355544ms step_avg:1245.90ms
step:1099/1390 train_time:1356986ms step_avg:1246.08ms
step:1100/1390 train_time:1358375ms step_avg:1246.22ms
step:1101/1390 train_time:1359742ms step_avg:1246.33ms
step:1102/1390 train_time:1361111ms step_avg:1246.44ms
step:1103/1390 train_time:1362490ms step_avg:1246.56ms
step:1104/1390 train_time:1363970ms step_avg:1246.77ms
step:1105/1390 train_time:1365357ms step_avg:1246.90ms
step:1106/1390 train_time:1366730ms step_avg:1247.02ms
step:1107/1390 train_time:1368119ms step_avg:1247.15ms
step:1108/1390 train_time:1369542ms step_avg:1247.31ms
step:1109/1390 train_time:1370927ms step_avg:1247.43ms
step:1110/1390 train_time:1372329ms step_avg:1247.57ms
step:1111/1390 train_time:1373653ms step_avg:1247.64ms
step:1112/1390 train_time:1375030ms step_avg:1247.76ms
step:1113/1390 train_time:1376384ms step_avg:1247.85ms
step:1114/1390 train_time:1377736ms step_avg:1247.95ms
step:1115/1390 train_time:1379095ms step_avg:1248.05ms
step:1116/1390 train_time:1380517ms step_avg:1248.21ms
step:1117/1390 train_time:1381899ms step_avg:1248.33ms
step:1118/1390 train_time:1383223ms step_avg:1248.40ms
step:1119/1390 train_time:1384669ms step_avg:1248.57ms
step:1120/1390 train_time:1386194ms step_avg:1248.82ms
step:1121/1390 train_time:1387565ms step_avg:1248.93ms
step:1122/1390 train_time:1388940ms step_avg:1249.05ms
step:1123/1390 train_time:1390293ms step_avg:1249.14ms
step:1124/1390 train_time:1391686ms step_avg:1249.27ms
step:1125/1390 train_time:1393089ms step_avg:1249.41ms
step:1125/1390 val_loss:3.3553 train_time:1393389ms step_avg:1249.68ms
step:1126/1390 train_time:1394411ms step_avg:1249.47ms
step:1127/1390 train_time:1395768ms step_avg:1249.57ms
step:1128/1390 train_time:1397172ms step_avg:1249.71ms
step:1129/1390 train_time:1398642ms step_avg:1249.90ms
step:1130/1390 train_time:1399990ms step_avg:1249.99ms
step:1131/1390 train_time:1401444ms step_avg:1250.17ms
step:1132/1390 train_time:1402834ms step_avg:1250.30ms
step:1133/1390 train_time:1404276ms step_avg:1250.47ms
step:1134/1390 train_time:1405620ms step_avg:1250.55ms
step:1135/1390 train_time:1407026ms step_avg:1250.69ms
step:1136/1390 train_time:1408363ms step_avg:1250.77ms
step:1137/1390 train_time:1409760ms step_avg:1250.90ms
step:1138/1390 train_time:1411293ms step_avg:1251.15ms
step:1139/1390 train_time:1412632ms step_avg:1251.22ms
step:1140/1390 train_time:1414018ms step_avg:1251.34ms
step:1141/1390 train_time:1415448ms step_avg:1251.50ms
step:1142/1390 train_time:1416869ms step_avg:1251.65ms
step:1143/1390 train_time:1418267ms step_avg:1251.78ms
step:1144/1390 train_time:1419765ms step_avg:1252.00ms
step:1145/1390 train_time:1421261ms step_avg:1252.21ms
step:1146/1390 train_time:1422580ms step_avg:1252.27ms
step:1147/1390 train_time:1423780ms step_avg:1252.22ms
step:1148/1390 train_time:1425300ms step_avg:1252.46ms
step:1149/1390 train_time:1426798ms step_avg:1252.68ms
step:1150/1390 train_time:1428214ms step_avg:1252.82ms
step:1151/1390 train_time:1429432ms step_avg:1252.79ms
step:1152/1390 train_time:1430703ms step_avg:1252.80ms
step:1153/1390 train_time:1431942ms step_avg:1252.79ms
step:1154/1390 train_time:1433163ms step_avg:1252.76ms
step:1155/1390 train_time:1434372ms step_avg:1252.73ms
step:1156/1390 train_time:1435724ms step_avg:1252.81ms
step:1157/1390 train_time:1436964ms step_avg:1252.80ms
step:1158/1390 train_time:1438301ms step_avg:1252.88ms
step:1159/1390 train_time:1439613ms step_avg:1252.93ms
step:1160/1390 train_time:1440864ms step_avg:1252.92ms
step:1161/1390 train_time:1442137ms step_avg:1252.94ms
step:1162/1390 train_time:1443406ms step_avg:1252.96ms
step:1163/1390 train_time:1444676ms step_avg:1252.97ms
step:1164/1390 train_time:1446013ms step_avg:1253.04ms
step:1165/1390 train_time:1447268ms step_avg:1253.05ms
step:1166/1390 train_time:1448681ms step_avg:1253.18ms
step:1167/1390 train_time:1450073ms step_avg:1253.30ms
step:1168/1390 train_time:1451518ms step_avg:1253.47ms
step:1169/1390 train_time:1452909ms step_avg:1253.59ms
step:1170/1390 train_time:1454325ms step_avg:1253.73ms
step:1171/1390 train_time:1455742ms step_avg:1253.87ms
step:1172/1390 train_time:1457166ms step_avg:1254.02ms
step:1173/1390 train_time:1458551ms step_avg:1254.13ms
step:1174/1390 train_time:1459933ms step_avg:1254.24ms
step:1175/1390 train_time:1461399ms step_avg:1254.42ms
step:1176/1390 train_time:1462805ms step_avg:1254.55ms
step:1177/1390 train_time:1464157ms step_avg:1254.63ms
step:1178/1390 train_time:1465555ms step_avg:1254.76ms
step:1179/1390 train_time:1467054ms step_avg:1254.96ms
step:1180/1390 train_time:1468476ms step_avg:1255.11ms
step:1181/1390 train_time:1469828ms step_avg:1255.19ms
step:1182/1390 train_time:1471311ms step_avg:1255.39ms
step:1183/1390 train_time:1472770ms step_avg:1255.56ms
step:1184/1390 train_time:1474167ms step_avg:1255.68ms
step:1185/1390 train_time:1475510ms step_avg:1255.75ms
step:1186/1390 train_time:1476972ms step_avg:1255.93ms
step:1187/1390 train_time:1478401ms step_avg:1256.08ms
step:1188/1390 train_time:1479762ms step_avg:1256.16ms
step:1189/1390 train_time:1481202ms step_avg:1256.32ms
step:1190/1390 train_time:1482598ms step_avg:1256.44ms
step:1191/1390 train_time:1484081ms step_avg:1256.63ms
step:1192/1390 train_time:1485427ms step_avg:1256.71ms
step:1193/1390 train_time:1486808ms step_avg:1256.81ms
step:1194/1390 train_time:1488255ms step_avg:1256.97ms
step:1195/1390 train_time:1489670ms step_avg:1257.11ms
step:1196/1390 train_time:1491088ms step_avg:1257.24ms
step:1197/1390 train_time:1492404ms step_avg:1257.29ms
step:1198/1390 train_time:1493794ms step_avg:1257.40ms
step:1199/1390 train_time:1495230ms step_avg:1257.55ms
step:1200/1390 train_time:1496704ms step_avg:1257.73ms
step:1201/1390 train_time:1498139ms step_avg:1257.88ms
step:1202/1390 train_time:1499593ms step_avg:1258.05ms
step:1203/1390 train_time:1500992ms step_avg:1258.17ms
step:1204/1390 train_time:1502394ms step_avg:1258.29ms
step:1205/1390 train_time:1503977ms step_avg:1258.56ms
step:1206/1390 train_time:1505393ms step_avg:1258.69ms
step:1207/1390 train_time:1506755ms step_avg:1258.78ms
step:1208/1390 train_time:1508172ms step_avg:1258.91ms
step:1209/1390 train_time:1509630ms step_avg:1259.07ms
step:1210/1390 train_time:1510992ms step_avg:1259.16ms
step:1211/1390 train_time:1512461ms step_avg:1259.33ms
step:1212/1390 train_time:1513910ms step_avg:1259.49ms
step:1213/1390 train_time:1515188ms step_avg:1259.51ms
step:1214/1390 train_time:1516505ms step_avg:1259.56ms
step:1215/1390 train_time:1518069ms step_avg:1259.81ms
step:1216/1390 train_time:1519521ms step_avg:1259.97ms
step:1217/1390 train_time:1520796ms step_avg:1259.98ms
step:1218/1390 train_time:1522050ms step_avg:1259.97ms
step:1219/1390 train_time:1523312ms step_avg:1259.98ms
step:1220/1390 train_time:1524525ms step_avg:1259.94ms
step:1221/1390 train_time:1525791ms step_avg:1259.94ms
step:1222/1390 train_time:1527026ms step_avg:1259.92ms
step:1223/1390 train_time:1528266ms step_avg:1259.91ms
step:1224/1390 train_time:1529560ms step_avg:1259.93ms
step:1225/1390 train_time:1530816ms step_avg:1259.93ms
step:1226/1390 train_time:1532040ms step_avg:1259.90ms
step:1227/1390 train_time:1533358ms step_avg:1259.95ms
step:1228/1390 train_time:1534667ms step_avg:1259.99ms
step:1229/1390 train_time:1535973ms step_avg:1260.03ms
step:1230/1390 train_time:1537213ms step_avg:1260.01ms
step:1231/1390 train_time:1538480ms step_avg:1260.02ms
step:1232/1390 train_time:1539885ms step_avg:1260.13ms
step:1233/1390 train_time:1541194ms step_avg:1260.17ms
step:1234/1390 train_time:1542665ms step_avg:1260.35ms
step:1235/1390 train_time:1544161ms step_avg:1260.54ms
step:1236/1390 train_time:1545530ms step_avg:1260.63ms
step:1237/1390 train_time:1546904ms step_avg:1260.72ms
step:1238/1390 train_time:1548271ms step_avg:1260.81ms
step:1239/1390 train_time:1549684ms step_avg:1260.93ms
step:1240/1390 train_time:1551049ms step_avg:1261.02ms
step:1241/1390 train_time:1552487ms step_avg:1261.16ms
step:1242/1390 train_time:1553898ms step_avg:1261.28ms
step:1243/1390 train_time:1555270ms step_avg:1261.37ms
step:1244/1390 train_time:1556673ms step_avg:1261.49ms
step:1245/1390 train_time:1558146ms step_avg:1261.66ms
step:1246/1390 train_time:1559564ms step_avg:1261.78ms
step:1247/1390 train_time:1560938ms step_avg:1261.87ms
step:1248/1390 train_time:1562321ms step_avg:1261.97ms
step:1249/1390 train_time:1563756ms step_avg:1262.11ms
step:1250/1390 train_time:1565140ms step_avg:1262.21ms
step:1250/1390 val_loss:3.3084 train_time:1565467ms step_avg:1262.47ms
step:1251/1390 train_time:1566485ms step_avg:1262.28ms
step:1252/1390 train_time:1567883ms step_avg:1262.39ms
step:1253/1390 train_time:1569274ms step_avg:1262.49ms
step:1254/1390 train_time:1570658ms step_avg:1262.59ms
step:1255/1390 train_time:1572091ms step_avg:1262.72ms
step:1256/1390 train_time:1573396ms step_avg:1262.76ms
step:1257/1390 train_time:1574905ms step_avg:1262.96ms
step:1258/1390 train_time:1576325ms step_avg:1263.08ms
step:1259/1390 train_time:1577700ms step_avg:1263.17ms
step:1260/1390 train_time:1579098ms step_avg:1263.28ms
step:1261/1390 train_time:1580590ms step_avg:1263.46ms
step:1262/1390 train_time:1581915ms step_avg:1263.51ms
step:1263/1390 train_time:1583313ms step_avg:1263.62ms
step:1264/1390 train_time:1584796ms step_avg:1263.79ms
step:1265/1390 train_time:1586235ms step_avg:1263.93ms
step:1266/1390 train_time:1587673ms step_avg:1264.07ms
step:1267/1390 train_time:1589141ms step_avg:1264.23ms
step:1268/1390 train_time:1590565ms step_avg:1264.36ms
step:1269/1390 train_time:1591980ms step_avg:1264.48ms
step:1270/1390 train_time:1593406ms step_avg:1264.61ms
step:1271/1390 train_time:1594785ms step_avg:1264.70ms
step:1272/1390 train_time:1596382ms step_avg:1264.96ms
step:1273/1390 train_time:1597800ms step_avg:1265.08ms
step:1274/1390 train_time:1599172ms step_avg:1265.17ms
step:1275/1390 train_time:1600510ms step_avg:1265.22ms
step:1276/1390 train_time:1601887ms step_avg:1265.31ms
step:1277/1390 train_time:1603302ms step_avg:1265.43ms
step:1278/1390 train_time:1604741ms step_avg:1265.57ms
step:1279/1390 train_time:1606148ms step_avg:1265.68ms
step:1280/1390 train_time:1607512ms step_avg:1265.76ms
step:1281/1390 train_time:1608922ms step_avg:1265.87ms
step:1282/1390 train_time:1610416ms step_avg:1266.05ms
step:1283/1390 train_time:1611927ms step_avg:1266.24ms
step:1284/1390 train_time:1613345ms step_avg:1266.36ms
step:1285/1390 train_time:1614746ms step_avg:1266.47ms
step:1286/1390 train_time:1616176ms step_avg:1266.60ms
step:1287/1390 train_time:1617527ms step_avg:1266.66ms
step:1288/1390 train_time:1618966ms step_avg:1266.80ms
step:1289/1390 train_time:1620374ms step_avg:1266.91ms
step:1290/1390 train_time:1621808ms step_avg:1267.04ms
step:1291/1390 train_time:1623136ms step_avg:1267.09ms
step:1292/1390 train_time:1624545ms step_avg:1267.20ms
step:1293/1390 train_time:1626127ms step_avg:1267.44ms
step:1294/1390 train_time:1627494ms step_avg:1267.52ms
step:1295/1390 train_time:1628883ms step_avg:1267.61ms
step:1296/1390 train_time:1630278ms step_avg:1267.71ms
step:1297/1390 train_time:1631648ms step_avg:1267.79ms
step:1298/1390 train_time:1633103ms step_avg:1267.94ms
step:1299/1390 train_time:1634614ms step_avg:1268.13ms
step:1300/1390 train_time:1635977ms step_avg:1268.20ms
step:1301/1390 train_time:1637313ms step_avg:1268.25ms
step:1302/1390 train_time:1638743ms step_avg:1268.38ms
step:1303/1390 train_time:1640088ms step_avg:1268.44ms
step:1304/1390 train_time:1641517ms step_avg:1268.56ms
step:1305/1390 train_time:1642897ms step_avg:1268.65ms
step:1306/1390 train_time:1644287ms step_avg:1268.74ms
step:1307/1390 train_time:1645610ms step_avg:1268.78ms
step:1308/1390 train_time:1647030ms step_avg:1268.90ms
step:1309/1390 train_time:1648442ms step_avg:1269.01ms
step:1310/1390 train_time:1649812ms step_avg:1269.09ms
step:1311/1390 train_time:1651230ms step_avg:1269.20ms
step:1312/1390 train_time:1652643ms step_avg:1269.31ms
step:1313/1390 train_time:1654060ms step_avg:1269.42ms
step:1314/1390 train_time:1655419ms step_avg:1269.49ms
step:1315/1390 train_time:1656839ms step_avg:1269.61ms
step:1316/1390 train_time:1658220ms step_avg:1269.69ms
step:1317/1390 train_time:1659639ms step_avg:1269.81ms
step:1318/1390 train_time:1661094ms step_avg:1269.95ms
step:1319/1390 train_time:1662496ms step_avg:1270.05ms
step:1320/1390 train_time:1663863ms step_avg:1270.12ms
step:1321/1390 train_time:1665391ms step_avg:1270.32ms
step:1322/1390 train_time:1666753ms step_avg:1270.39ms
step:1323/1390 train_time:1668214ms step_avg:1270.54ms
step:1324/1390 train_time:1669572ms step_avg:1270.60ms
step:1325/1390 train_time:1670869ms step_avg:1270.62ms
step:1326/1390 train_time:1672250ms step_avg:1270.71ms
step:1327/1390 train_time:1673612ms step_avg:1270.78ms
step:1328/1390 train_time:1675125ms step_avg:1270.96ms
step:1329/1390 train_time:1676613ms step_avg:1271.12ms
step:1330/1390 train_time:1678044ms step_avg:1271.25ms
step:1331/1390 train_time:1679564ms step_avg:1271.43ms
step:1332/1390 train_time:1680920ms step_avg:1271.50ms
step:1333/1390 train_time:1682317ms step_avg:1271.59ms
step:1334/1390 train_time:1683727ms step_avg:1271.70ms
step:1335/1390 train_time:1685228ms step_avg:1271.87ms
step:1336/1390 train_time:1686567ms step_avg:1271.92ms
step:1337/1390 train_time:1687943ms step_avg:1272.00ms
step:1338/1390 train_time:1689361ms step_avg:1272.11ms
step:1339/1390 train_time:1690842ms step_avg:1272.27ms
step:1340/1390 train_time:1692239ms step_avg:1272.36ms
step:1341/1390 train_time:1693672ms step_avg:1272.48ms
step:1342/1390 train_time:1695112ms step_avg:1272.61ms
step:1343/1390 train_time:1696535ms step_avg:1272.72ms
step:1344/1390 train_time:1697993ms step_avg:1272.86ms
step:1345/1390 train_time:1699491ms step_avg:1273.03ms
step:1346/1390 train_time:1700891ms step_avg:1273.12ms
step:1347/1390 train_time:1702251ms step_avg:1273.19ms
step:1348/1390 train_time:1703766ms step_avg:1273.37ms
step:1349/1390 train_time:1705213ms step_avg:1273.50ms
step:1350/1390 train_time:1706690ms step_avg:1273.65ms
step:1351/1390 train_time:1708112ms step_avg:1273.76ms
step:1352/1390 train_time:1709541ms step_avg:1273.88ms
step:1353/1390 train_time:1710893ms step_avg:1273.93ms
step:1354/1390 train_time:1712333ms step_avg:1274.06ms
step:1355/1390 train_time:1713862ms step_avg:1274.25ms
step:1356/1390 train_time:1715358ms step_avg:1274.41ms
step:1357/1390 train_time:1716801ms step_avg:1274.54ms
step:1358/1390 train_time:1718220ms step_avg:1274.64ms
step:1359/1390 train_time:1719623ms step_avg:1274.74ms
step:1360/1390 train_time:1721141ms step_avg:1274.92ms
step:1361/1390 train_time:1722648ms step_avg:1275.09ms
step:1362/1390 train_time:1724054ms step_avg:1275.19ms
step:1363/1390 train_time:1725510ms step_avg:1275.32ms
step:1364/1390 train_time:1726994ms step_avg:1275.48ms
step:1365/1390 train_time:1728335ms step_avg:1275.52ms
step:1366/1390 train_time:1729851ms step_avg:1275.70ms
step:1367/1390 train_time:1731237ms step_avg:1275.78ms
step:1368/1390 train_time:1732620ms step_avg:1275.86ms
step:1369/1390 train_time:1734100ms step_avg:1276.01ms
step:1370/1390 train_time:1735535ms step_avg:1276.13ms
step:1371/1390 train_time:1736957ms step_avg:1276.24ms
step:1372/1390 train_time:1738497ms step_avg:1276.43ms
step:1373/1390 train_time:1739829ms step_avg:1276.47ms
step:1374/1390 train_time:1741236ms step_avg:1276.57ms
step:1375/1390 train_time:1742781ms step_avg:1276.76ms
step:1375/1390 val_loss:3.2794 train_time:1743120ms step_avg:1277.01ms
step:1376/1390 train_time:1744235ms step_avg:1276.89ms
step:1377/1390 train_time:1745720ms step_avg:1277.04ms
step:1378/1390 train_time:1747146ms step_avg:1277.15ms
step:1379/1390 train_time:1748515ms step_avg:1277.22ms
step:1380/1390 train_time:1749984ms step_avg:1277.36ms
step:1381/1390 train_time:1751437ms step_avg:1277.49ms
step:1382/1390 train_time:1752896ms step_avg:1277.62ms
step:1383/1390 train_time:1754167ms step_avg:1277.62ms
step:1384/1390 train_time:1755654ms step_avg:1277.77ms
step:1385/1390 train_time:1757047ms step_avg:1277.85ms
step:1386/1390 train_time:1758456ms step_avg:1277.95ms
step:1387/1390 train_time:1760027ms step_avg:1278.16ms
step:1388/1390 train_time:1761398ms step_avg:1278.23ms
step:1389/1390 train_time:1762803ms step_avg:1278.32ms
step:1390/1390 train_time:1764179ms step_avg:1278.39ms
step:1390/1390 val_loss:3.2786 train_time:1764507ms step_avg:1278.63ms
peak memory consumption: 31533 MiB
